function [theta] = sgd_dnn(theta, ei, X, y, lr)
  %
  % Arguments:
  %   theta - A vector containing the parameter values to optimize.
  %       In minFunc, theta is reshaped to a long vector.  So we need to
  %       resize it to an n-by-(num_classes-1) matrix.
  %       Recall that we assume theta(:,num_classes) = 0.
  %
  %   X - The examples stored in a matrix.  
  %       X(i,j) is the i'th coordinate of the j'th example.
  %   y - The label for each example.  y(j) is the j'th example's label.
  %
  
  stack = params2stack(theta, ei);
  numHidden = numel(ei.layer_sizes) - 1;
  hAct = cell(numHidden+1, 1);
  gradStack = cell(numHidden+1, 1);
  delta = cell(numHidden+1,1);
  
  I = randperm(length(y));
  y=y(I); % labels in range 1 to 10
  X=X(:,I);
  
%f_before = Inf;
iter = 0;
m = size(X,2);
%step = 1;
 = inf;
while(iter < 1000*m)
    i = rem(iter, m) + 1;
    data = X(:,i);
    labels = y(i);
    for i = 1:numHidden
        W = stack{i,1}.W;
        b = stack{i,1}.b;
        hAct{i} = sigmoid(bsxfun(@plus, W*data, b));
        %if hidden, then hAct is after nonlinear activation

    end
    W = stack{end,1}.W;
    b = stack{end,1}.b;
    hAct{end} = bsxfun(@plus,W*hAct{end-1},b);
    temp = exp(hAct{end});
    pred_prob = bsxfun(@rdivide, temp, sum(temp,1));

    %% compute cost
    %%% YOUR CODE HERE %%%
    I = sub2ind(size(pred_prob),labels',1:size(pred_prob,2));
    cost = -sum(log(pred_prob(I)));

    %% compute gradients using backpropagation
    temp = pred_prob;
    temp(I) = temp(I) - 1;
    delta{end} = temp;
    for i = 1:numHidden
        W = stack{end - i + 1}.W;
        temp = W' * delta{end- i + 1};
        g_prime = hAct{end-i}.*(1-hAct{end-i});
        delta{end-i} = g_prime.*temp;
    end

    %% compute weight penalty cost and gradient for non-bias terms
    %%% YOUR CODE HERE %%%
    gradStack{1}.W = delta{1}*data';
    gradStack{1}.b = sum(delta{1},2);
    for i=2:numHidden+1
       gradStack{i}.W = delta{i}*hAct{i-1}';
       gradStack{i}.b = sum(delta{i},2);
    end
    
    for i=1:numel(stack)
        stack{i}.W = stack{i}.W - lr * gradStack{i}.W;
        stack{i}.b = stack{i}.b - lr * gradStack{i}.b;
    end

    if rem(iter, 100)==0
        fprintf('Iteration: %d, Objective function: %f, lr: %e\n', iter, cost, lr);
    end
    if rem(iter,1000)==0
        for i = 1:numHidden
            W = stack{i,1}.W;
            b = stack{i,1}.b;
            hAct{i} = sigmoid(bsxfun(@plus, W*X, b));
            %if hidden, then hAct is after nonlinear activation
        end
        W = stack{end,1}.W;
        b = stack{end,1}.b;
        hAct{end} = bsxfun(@plus,W*hAct{end-1},b);
        temp = exp(hAct{end});
        pred_prob = bsxfun(@rdivide, temp, sum(temp,1));

        I = sub2ind(size(pred_prob),y',1:size(pred_prob,2));
        cost = -sum(log(pred_prob(I)));
        fprintf('BATCH Objective function: %f \n', cost);
    end
    iter = iter + 1;
end
[theta] = stack2params(stack);

end
